# prepare（必须）:
#   1、程序运行前，首先需要初始化程序数据表
#   2、配置 reverse 自定义转换规则
#   - 优先级：表字段类型 > 库字段类型 两者都没配置默认采用内置转换规则
# reverse:
#   1、prepare 前提必须阶段
#   2、根据内置表结构转换规则或者手工配置表结构转换规则进行 schema 迁移
# gather:
#   1、用于收集评估 oracle -> mysql/tidb 迁移成本信息，只适用于 schema 级别
# check:
#   1、表结构检查(独立于表结构转换，可单独运行，校验规则使用内置规则)
# all:（全量 + 增量模式）
#   1、全量数据迁移
#   2、增量数据迁移
# full: (全量模式)
#   1、全量数据迁移 -> REPLACE INTO
# csv：（全量模式）
#   1、全量数据导出 -> CSV
[app]
# 事务 batch 数
# 用于数据写入 batch 提交事务数
insert-batch-size = 500
# 是否开启更新元数据库表慢日志，单位毫秒
slowlog-threshold = 300
# 任务并发 -- 只用于 reverse/check 模式阶段
threads = 256
# pprof 端口
pprof-port = ":9696"
# http
http-port = "9000"
#
jwt-secret = "666888"


# JwtSecret string

[diff]
chunk-size = 1000
# 检查数据并发数
diff-threads = 4
# 只检查数据行数
# 设置 true 代表只检查数据行数，设置 false 代表使用 checksum 数据对比以及输出对应差异数据
only-check-rows = false
# 断点续检，代表从上次 checkpoint 开始检查
enable-checkpoint = true
# 忽略表结构、collation 以及 character 检查，数据校验是否校验表结构，以上游表结构为准
ignore-struct-check = true
# 差异修复 SQL 文件, ONLY 用于下游数据库变更修复
fix-sql-file = "fix.sql"

# diff 某些表单独配置 -> 源端表
#[[table-config]]
# 源端表
#source-table = "marvin"
# 指定 NUMBER 类型字段，必须带索引
#index-fields = "id"
# 指定检查数据范围或者查询条件
# range 优先级高于 index-fields
#range = "age > 10 AND age< 20"

[csv]
# CSV 文件是否包含表头
header = true
# 字段分隔符，支持一个或多个字符，默认值为 ','
separator = '|#|'
# 行尾定界字符，支持一个或多个字符, 默认值 "\r\n" （回车+换行）
terminator = "|+|\r\n"
# 字符串引用定界符，支持一个或多个字符，设置为空表示字符串未加引号
delimiter = '"'
# 使用反斜杠 (\) 来转义导出文件中的特殊字符
escape-backslash = true
# 目标数据库字符集 utf8/gbk，设置为空表示以上游数据库为准
charset = "utf8"
# 1、任务行数数，固定动作，一旦确认，不能更改，除非设置 enable-checkpoint = false，重新导出导入
# 2、代表每张表每并发处理多少行数
# 3、代表多少行数据切分一个 csv 文件
# 4、建议是 insert-batch-size 整数倍
rows = 100000
# 数据文件输出目录, 所有表数据输出文件目录，需要磁盘空间充足
output-dir = "/tmp"
# 用于初始化表任务并发数【写下游 meta 数据库】
task-threads = 128
# 表导出导入并发数，同时处理多少张上游表，可动态变更
table-threads = 8
# 1、单表 SQL 执行并发数，表内并发，表示同时多少并发 SQL 读取上游表数据，可动态变更
# 2、单表 csv 并发写线程数，表示同时多少个 csv 文件同时写，可动态变更
sql-threads = 64
# 关于全量断点恢复
#   - 若想断点恢复，设置 enable-checkpoint = true,首次一旦运行则 chunk-size 数不能调整，
#   - 若不想断点恢复或者重新调整 chunk-size 数，设置 enable-checkpoint = false,重新运行全量任务
#   - 无法断点续传期间，则需要设置 enable-checkpoint = false 重新导入导出
enable-checkpoint = true
[full]
# 表间串行，表内并发
# 任务 chunk 数，固定动作，一旦确认，不能更改，除非设置 enable-checkpoint = false，重新导出导入
# 1、代表每张表每并发处理多少行数
# 2、建议参数值是 insert-batch-size 整数倍，会根据 insert-batch-size 大小切分
chunk-size = 100000
# 用于初始化表任务并发数【写下游 meta 数据库】
task-threads = 128
# 表导出导入并发数，同时处理多少张上游表，可动态变更
table-threads = 8
# 单表 SQL 执行并发数，表示同时多少并发 SQL 读取上游表数据，可动态变更
sql-threads = 64
# 每 sql-threads 线程写下游并发数，可动态变更
apply-threads = 128
# 关于全量断点恢复
#   - 若想断点恢复，设置 enable-checkpoint = true,首次一旦运行则 chunk-size 数不能调整，
#   - 若不想断点恢复或者重新调整 chunk-size 数，设置 enable-checkpoint = false,重新运行全量任务
#   - 无法断点续传期间，则需要设置 enable-checkpoint = false 重新导入导出
enable-checkpoint = true

[all]
# logminer 挖掘最长耗时，单位: 秒
logminer-query-timeout   = 300
# 并发筛选 oracle 日志数
filter-threads = 16
# 并发表应用数，同时处理多少张表
apply-threads = 3
# apply-threads 每个表并发处理最大工作对列
worker-queue = 3
# apply-threads 每个表并发处理最大任务分发数
worker-threads = 3

[source]
# 源端 oracle 连接串
# 若需要增量数据同步，则需要 logminer 权限
username = "test"
password = "test"
host = "192.168.1.122"
port = 1521
service-name = "helowin"
# 配置 oracle 连接参数
connect-params = "poolMinSessions=10&poolMaxSessions=1000&poolWaitTimeout=60s&poolSessionMaxLifetime=1h&poolSessionTimeout=5m&poolIncrement=10&timezone=Local"
# 配置 oracle 连接会话 session 变量
session-params = ["alter session set nls_date_format = 'yyyy-mm-dd hh24:mi:ss'"]
# 配置 oracle 迁移 schema（gather 阶段可设置可不设置，不设置则表示 gather 库内所有 schema，其他阶段必须设置）
schema-name = "test"
# 源端迁移任务表（只用于 prepare/reverse/check/all/full 阶段，gather 阶段不适用，gather 只适用于 schema 级别）
# include-table 和 exclude-table 不能同时配置，两者只能配置一个,如果两个都没配置则 Schema 内表全迁移
include-table = ["STS0"]
exclude-table = []

# 只用于 prepare/reverse/check/all/full 阶段，gather 阶段不适用
[target]
# 目标端 mysql 连接串
username = "root"
password = "123456"
host = "192.168.1.122"
port = 3306
# mysql 链接参数
# 数据库时间字符串化，'2022-03-30T15:06:09+08:00' -> '2022-03-30T15:06:09'，数据库连接参数禁止配置 parseTime=True&loc=Local
connect-params = "charset=utf8mb4&multiStatements=true"
# 目标端元数据库
# CREATE DATABASE IF NOT EXIST db_meta
meta-schema = "db_meta"
# 目标端导入 schema
schema-name = "todb"


[log]
# 日志 level
log-level = "info"
# 日志文件路径
log-file = "./conf/transferdb.log"
# 每个日志文件保存的最大尺寸 单位：M
max-size = 128
# 文件最多保存多少天
max-days = 7
# 日志文件最多保存多少个备份
max-backups = 30